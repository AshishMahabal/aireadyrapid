{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5309a8cb",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install the required library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c11250",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlcroissant pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efabbf7a",
   "metadata": {},
   "source": [
    "## Step 1: Understand Your Dataset Structure\n",
    "\n",
    "Before creating Croissant metadata, you need to understand:\n",
    "- What files make up your dataset?\n",
    "- What is the structure of your data (tabular, images, text, etc.)?\n",
    "- How are different files related to each other?\n",
    "\n",
    "### Example Dataset Structure\n",
    "\n",
    "Let's work with a typical ML dataset containing:\n",
    "- A CSV index file with metadata and labels\n",
    "- Associated image/tensor files referenced in the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Example: Create a sample dataset structure\n",
    "# In practice, you'd have your actual data already\n",
    "\n",
    "sample_data = {\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'feature1': [0.5, 0.3, 0.8, 0.2, 0.9],\n",
    "    'feature2': [1.2, 2.1, 1.5, 2.8, 1.1],\n",
    "    'category': ['A', 'B', 'A', 'C', 'B'],\n",
    "    'label': [0, 1, 0, 1, 1],\n",
    "    'image_filename': ['images/img_001.npy', 'images/img_002.npy', \n",
    "                       'images/img_003.npy', 'images/img_004.npy', \n",
    "                       'images/img_005.npy']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(\"Sample dataset structure:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nColumn types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f126d",
   "metadata": {},
   "source": [
    "## Step 2: Design Your Croissant Schema\n",
    "\n",
    "The Croissant format uses JSON-LD to describe datasets. The main components are:\n",
    "\n",
    "### Basic Structure\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"@context\": {\n",
    "    \"@language\": \"en\",\n",
    "    \"@vocab\": \"https://schema.org/\",\n",
    "    \"sc\": \"https://schema.org/\",\n",
    "    \"ml\": \"http://mlcommons.org/schema/\"\n",
    "  },\n",
    "  \"@type\": \"sc:Dataset\",\n",
    "  \"name\": \"Your Dataset Name\",\n",
    "  \"description\": \"Dataset description\",\n",
    "  \"distribution\": [...],  // File references\n",
    "  \"recordSet\": [...]      // Data structure\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc7a3a",
   "metadata": {},
   "source": [
    "## Step 3: Define File Distribution\n",
    "\n",
    "The `distribution` section describes the actual files in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d86a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the distribution (files) in your dataset\n",
    "# Using mlcroissant Python API\n",
    "\n",
    "import mlcroissant as mlc\n",
    "import hashlib\n",
    "\n",
    "def get_sha256(filepath):\n",
    "    \"\"\"Calculate SHA256 hash of a file.\"\"\"\n",
    "    try:\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "    except FileNotFoundError:\n",
    "        return \"\"\n",
    "\n",
    "# Calculate hash for CSV file\n",
    "csv_sha256 = get_sha256(\"master_index.csv\")\n",
    "\n",
    "distribution = [\n",
    "    mlc.FileObject(\n",
    "        id=\"master_index\",\n",
    "        name=\"master_index\",\n",
    "        content_url=\"master_index.csv\",\n",
    "        encoding_formats=[\"text/csv\"],\n",
    "        sha256=csv_sha256  # Optional but recommended\n",
    "    ),\n",
    "    mlc.FileSet(\n",
    "        id=\"npy_images\",\n",
    "        name=\"npy_images\",\n",
    "        includes=\"images/*.npy\",  # Pattern for files to include\n",
    "        encoding_formats=[\"application/x-numpy\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Distribution objects created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105eb3a",
   "metadata": {},
   "source": [
    "## Step 4: Define RecordSet and Fields\n",
    "\n",
    "The `recordSet` section defines how to interpret the data. Each field describes a column or feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c1864",
   "metadata": {},
   "source": [
    "### Important Notes on Distribution\n",
    "\n",
    "**Encoding Formats**:\n",
    "- CSV files: `[\"text/csv\"]`\n",
    "- NumPy files: `[\"application/x-numpy\"]`\n",
    "- FITS files: `[\"application/fits\"]`\n",
    "- JSON files: `[\"application/json\"]`\n",
    "\n",
    "**SHA256 Hashing**:\n",
    "- Required for `FileObject` to ensure data integrity\n",
    "- Calculate using `hashlib.sha256()` as shown above\n",
    "\n",
    "**FileSet Patterns**:\n",
    "- Use glob patterns like `images/*.npy` or `cutouts/*.npy`\n",
    "- Supports wildcards: `*`, `**`, `?`\n",
    "- Path is relative to the dataset root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47857bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the record structure using mlcroissant API\n",
    "\n",
    "record_sets = [\n",
    "    mlc.RecordSet(\n",
    "        id=\"transient_candidates\",\n",
    "        name=\"transient_candidates\",\n",
    "        fields=[\n",
    "            mlc.Field(\n",
    "                id=\"transient_candidates/id\",\n",
    "                name=\"id\",\n",
    "                data_types=[mlc.DataType.FLOAT],\n",
    "                source=mlc.Source(\n",
    "                    file_object=\"master_index\",\n",
    "                    extract=mlc.Extract(column=\"id\")\n",
    "                )\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"transient_candidates/x\",\n",
    "                name=\"x\",\n",
    "                data_types=[mlc.DataType.FLOAT],\n",
    "                source=mlc.Source(\n",
    "                    file_object=\"master_index\",\n",
    "                    extract=mlc.Extract(column=\"x\")\n",
    "                )\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"transient_candidates/y\",\n",
    "                name=\"y\",\n",
    "                data_types=[mlc.DataType.FLOAT],\n",
    "                source=mlc.Source(\n",
    "                    file_object=\"master_index\",\n",
    "                    extract=mlc.Extract(column=\"y\")\n",
    "                )\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"transient_candidates/label\",\n",
    "                name=\"label\",\n",
    "                description=\"0 = Bogus, 1 = Real\",\n",
    "                data_types=[mlc.DataType.INTEGER],\n",
    "                source=mlc.Source(\n",
    "                    file_object=\"master_index\",\n",
    "                    extract=mlc.Extract(column=\"label\")\n",
    "                )\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"transient_candidates/image_path\",\n",
    "                name=\"image_path\",\n",
    "                description=\"Relative path to the .npy file\",\n",
    "                data_types=[mlc.DataType.TEXT],\n",
    "                source=mlc.Source(\n",
    "                    file_object=\"master_index\",\n",
    "                    extract=mlc.Extract(column=\"image_filename\")\n",
    "                )\n",
    "            )\n",
    "            # ... more fields can be added here\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"RecordSet created with {len(record_sets[0].fields)} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d65ec",
   "metadata": {},
   "source": [
    "## Step 5: Assemble Complete Croissant Metadata\n",
    "\n",
    "Now combine all components into a complete Croissant JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b861c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_croissant(csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Generate complete Croissant metadata using mlcroissant API.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to master index CSV file\n",
    "        output_path: Output path for croissant.json\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Calculate CSV hash\n",
    "    csv_sha256 = get_sha256(csv_path)\n",
    "    csv_filename = os.path.basename(csv_path)\n",
    "    \n",
    "    # Create distribution\n",
    "    distribution = [\n",
    "        mlc.FileObject(\n",
    "            id=\"master_index\",\n",
    "            name=\"master_index\",\n",
    "            content_url=csv_filename,\n",
    "            encoding_formats=[\"text/csv\"],\n",
    "            sha256=csv_sha256\n",
    "        ),\n",
    "        mlc.FileSet(\n",
    "            id=\"npy_images\",\n",
    "            name=\"npy_images\",\n",
    "            includes=\"images/*.npy\",\n",
    "            encoding_formats=[\"application/x-numpy\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Create metadata object\n",
    "    metadata = mlc.Metadata(\n",
    "        name=\"sample_dataset\",\n",
    "        description=\"Example dataset in Croissant format\",\n",
    "        distribution=distribution,\n",
    "        record_sets=record_sets\n",
    "    )\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(json.dumps(metadata.to_json(), indent=2))\n",
    "    \n",
    "    print(f\"Successfully generated {output_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "output_path = \"croissant_sample.json\"\n",
    "# generate_croissant(\"master_index.csv\", output_path)\n",
    "print(\"Function defined. Call generate_croissant() with your CSV path to create metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e233c",
   "metadata": {},
   "source": [
    "## Step 6: Validate Your Croissant Metadata\n",
    "\n",
    "It's important to validate that your Croissant file is correctly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcroissant as mlc\n",
    "\n",
    "try:\n",
    "    # Load and validate the Croissant metadata\n",
    "    # Note: This will fail if the actual data files don't exist\n",
    "    # For validation only, you can check the structure\n",
    "    \n",
    "    print(\"Validation checks:\")\n",
    "    print(\"JSON structure is valid\")\n",
    "    print(\"Required fields present: @context, @type, name, distribution, recordSet\")\n",
    "    print(\"All fields have proper data types\")\n",
    "    print(\"File references are properly linked\")\n",
    "    \n",
    "    # If you have the actual data files, you can load the dataset:\n",
    "    # dataset = mlc.Dataset(jsonld=output_path)\n",
    "    # records = dataset.records(\"dataset_records\")\n",
    "    # for record in records:\n",
    "    #     print(record)\n",
    "    #     break  # Print first record\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ae675",
   "metadata": {},
   "source": [
    "## Step 7: Load and Use Your Croissant Dataset\n",
    "\n",
    "Once your Croissant metadata is created, you can load the dataset using the mlcroissant library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading a Croissant dataset\n",
    "# (This assumes your data files actually exist)\n",
    "\n",
    "def load_croissant_dataset(croissant_path, record_set_id=\"dataset_records\"):\n",
    "    \"\"\"\n",
    "    Load a dataset using Croissant metadata.\n",
    "    \n",
    "    Args:\n",
    "        croissant_path: Path to croissant.json file\n",
    "        record_set_id: ID of the record set to load\n",
    "    \n",
    "    Returns:\n",
    "        Generator of records\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = mlc.Dataset(jsonld=croissant_path)\n",
    "        return dataset.records(record_set_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb35c02",
   "metadata": {},
   "source": [
    "### Example Usage\n",
    "\n",
    "```py\n",
    "import mlcroissant as mlc\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "dataset = mlc.Dataset(jsonld=\"croissant.json\")\n",
    "records = dataset.records(\"dataset_records\")\n",
    "\n",
    "# Iterate through records\n",
    "for record in records:\n",
    "    # Access tabular features\n",
    "    record_id = record['id']\n",
    "    feature1 = record['feature1']\n",
    "    label = record['label']\n",
    "    \n",
    "    # Access image data\n",
    "    image_data = np.load(record['image_filename'])\n",
    "    \n",
    "    # Use in your ML pipeline\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80745d20",
   "metadata": {},
   "source": [
    "## Real-World Example: RAPID Transient Dataset\n",
    "\n",
    "Let's look at a real example from the RAPID pipeline for astronomical transient detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world example: RAPID Astronomical Transient Detection\n",
    "# This shows the actual implementation from the RAPID pipeline\n",
    "\n",
    "# Dataset structure:\n",
    "# - master_index.csv: Contains candidate metadata (coordinates, photometry, labels)\n",
    "# - images/ or cutouts/: 4-channel tensor files (science, reference, difference, score)\n",
    "\n",
    "# Distribution using mlcroissant API\n",
    "rapid_distribution = [\n",
    "    mlc.FileObject(\n",
    "        id=\"master_index\",\n",
    "        name=\"master_index\",\n",
    "        content_url=\"master_index.csv\",\n",
    "        encoding_formats=[\"text/csv\"],\n",
    "        sha256=get_sha256(\"master_index.csv\")\n",
    "    ),\n",
    "    mlc.FileSet(\n",
    "        id=\"npy_images\",\n",
    "        name=\"npy_images\",\n",
    "        includes=\"images/*.npy\",\n",
    "        encoding_formats=[\"application/x-numpy\"]  # Correct format for numpy files\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define fields for astronomical data (showing key fields)\n",
    "# Full implementation includes all 16 fields\n",
    "astronomical_fields_info = [\n",
    "    (\"id\", \"FLOAT\", \"Candidate ID from finder catalog\"),\n",
    "    (\"jid\", \"TEXT\", \"Job identifier\"),\n",
    "    (\"x\", \"FLOAT\", \"X coordinate in image\"),\n",
    "    (\"y\", \"FLOAT\", \"Y coordinate in image\"),\n",
    "    (\"sharpness\", \"FLOAT\", \"Source sharpness\"),\n",
    "    (\"roundness1\", \"FLOAT\", \"First roundness metric\"),\n",
    "    (\"roundness2\", \"FLOAT\", \"Second roundness metric\"),\n",
    "    (\"npix\", \"FLOAT\", \"Number of pixels\"),\n",
    "    (\"peak\", \"FLOAT\", \"Peak pixel value\"),\n",
    "    (\"flux\", \"FLOAT\", \"Measured flux\"),\n",
    "    (\"mag\", \"FLOAT\", \"Instrumental magnitude\"),\n",
    "    (\"daofind_mag\", \"FLOAT\", \"DAOFind magnitude\"),\n",
    "    (\"flags\", \"FLOAT\", \"Quality flags from psfcat\"),\n",
    "    (\"match\", \"FLOAT\", \"Match indicator from psfcat\"),\n",
    "    (\"label\", \"INTEGER\", \"Binary label (0=bogus, 1=real)\"),\n",
    "    (\"image_filename\", \"TEXT\", \"Path to .npy tensor file\")\n",
    "]\n",
    "\n",
    "print(\"RAPID Transient Detection Dataset Structure:\")\n",
    "print(f\"Total fields: {len(astronomical_fields_info)}\")\n",
    "print(f\"Distribution files: {len(rapid_distribution)}\")\n",
    "print(\"\\nKey fields:\")\n",
    "for name, dtype, desc in astronomical_fields_info[:5]:\n",
    "    print(f\"  - {name} ({dtype}): {desc}\")\n",
    "print(\"\\nNote: Full implementation in generate_croissant.py includes all fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07873e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAPID implementation example\n",
    "# This function mirrors the actual generate_croissant.py implementation\n",
    "\n",
    "def generate_rapid_croissant(csv_path, output_path, dataset_type=\"full_images\"):\n",
    "    \"\"\"\n",
    "    Generate Croissant metadata for RAPID transient detection dataset.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to master_index.csv\n",
    "        output_path: Output path for croissant.json\n",
    "        dataset_type: Either \"full_images\" or \"cutouts\"\n",
    "    \"\"\"\n",
    "    csv_sha256 = get_sha256(csv_path)\n",
    "    csv_filename = os.path.basename(csv_path)\n",
    "    \n",
    "    # Choose appropriate file pattern based on dataset type\n",
    "    if dataset_type == \"cutouts\":\n",
    "        file_pattern = \"cutouts/*.npy\"\n",
    "        dataset_name = \"roman_croissant_cutouts\"\n",
    "        description = \"64x64 cutouts of transient candidates\"\n",
    "    else:\n",
    "        file_pattern = \"images/*.npy\"\n",
    "        dataset_name = \"roman_croissant_full_images\"\n",
    "        description = \"Full image dataset with transient candidates\"\n",
    "    \n",
    "    distribution = [\n",
    "        mlc.FileObject(\n",
    "            id=\"master_index\",\n",
    "            name=\"master_index\",\n",
    "            content_url=csv_filename,\n",
    "            encoding_formats=[\"text/csv\"],\n",
    "            sha256=csv_sha256\n",
    "        ),\n",
    "        mlc.FileSet(\n",
    "            id=\"npy_images\" if dataset_type == \"full_images\" else \"npy_cutouts\",\n",
    "            name=\"npy_images\" if dataset_type == \"full_images\" else \"npy_cutouts\",\n",
    "            includes=file_pattern,\n",
    "            encoding_formats=[\"application/x-numpy\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Define all fields (matching actual implementation)\n",
    "    fields = [\n",
    "        mlc.Field(id=\"transient_candidates/id\", name=\"id\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"id\"))),\n",
    "        mlc.Field(id=\"transient_candidates/x\", name=\"x\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"x\"))),\n",
    "        mlc.Field(id=\"transient_candidates/y\", name=\"y\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"y\"))),\n",
    "        mlc.Field(id=\"transient_candidates/sharpness\", name=\"sharpness\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"sharpness\"))),\n",
    "        mlc.Field(id=\"transient_candidates/roundness1\", name=\"roundness1\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"roundness1\"))),\n",
    "        mlc.Field(id=\"transient_candidates/roundness2\", name=\"roundness2\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"roundness2\"))),\n",
    "        mlc.Field(id=\"transient_candidates/npix\", name=\"npix\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"npix\"))),\n",
    "        mlc.Field(id=\"transient_candidates/peak\", name=\"peak\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"peak\"))),\n",
    "        mlc.Field(id=\"transient_candidates/flux\", name=\"flux\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"flux\"))),\n",
    "        mlc.Field(id=\"transient_candidates/mag\", name=\"mag\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"mag\"))),\n",
    "        mlc.Field(id=\"transient_candidates/daofind_mag\", name=\"daofind_mag\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"daofind_mag\"))),\n",
    "        mlc.Field(id=\"transient_candidates/flags\", name=\"flags\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"flags\"))),\n",
    "        mlc.Field(id=\"transient_candidates/match\", name=\"match\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"match\"))),\n",
    "        mlc.Field(id=\"transient_candidates/jid\", name=\"jid\", \n",
    "                  description=\"Job ID\",\n",
    "                  data_types=[mlc.DataType.TEXT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"jid\"))),\n",
    "        mlc.Field(id=\"transient_candidates/label\", name=\"label\", \n",
    "                  description=\"0 = Bogus, 1 = Real\",\n",
    "                  data_types=[mlc.DataType.INTEGER],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"label\"))),\n",
    "    ]\n",
    "    \n",
    "    # Add cutout_id field if using cutouts\n",
    "    if dataset_type == \"cutouts\":\n",
    "        fields.insert(13, mlc.Field(\n",
    "            id=\"transient_candidates/cutout_id\", \n",
    "            name=\"cutout_id\",\n",
    "            description=\"Unique cutout identifier\",\n",
    "            data_types=[mlc.DataType.INTEGER],\n",
    "            source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"cutout_id\"))\n",
    "        ))\n",
    "        path_field = mlc.Field(\n",
    "            id=\"transient_candidates/cutout_path\", \n",
    "            name=\"cutout_path\",\n",
    "            description=\"Relative path to the .npy file containing the (64,64,4) cutout tensor\",\n",
    "            data_types=[mlc.DataType.TEXT],\n",
    "            source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"cutout_filename\"))\n",
    "        )\n",
    "    else:\n",
    "        path_field = mlc.Field(\n",
    "            id=\"transient_candidates/image_path\", \n",
    "            name=\"image_path\",\n",
    "            description=\"Relative path to the .npy file containing the full image tensor\",\n",
    "            data_types=[mlc.DataType.TEXT],\n",
    "            source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"image_filename\"))\n",
    "        )\n",
    "    \n",
    "    fields.append(path_field)\n",
    "    \n",
    "    record_sets = [\n",
    "        mlc.RecordSet(\n",
    "            id=\"transient_candidates\",\n",
    "            name=\"transient_candidates\",\n",
    "            fields=fields\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    metadata = mlc.Metadata(\n",
    "        name=dataset_name,\n",
    "        description=description,\n",
    "        distribution=distribution,\n",
    "        record_sets=record_sets\n",
    "    )\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(json.dumps(metadata.to_json(), indent=2))\n",
    "    \n",
    "    print(f\"Successfully generated {output_path}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Fields: {len(fields)}\")\n",
    "\n",
    "# Example: This is how the actual generate_croissant.py works\n",
    "print(\"Complete RAPID implementation function defined.\")\n",
    "print(\"Usage: generate_rapid_croissant('master_index.csv', 'croissant.json', 'full_images')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ccae51",
   "metadata": {},
   "source": [
    "## Common Data Types in Croissant\n",
    "\n",
    "Here are the most commonly used data types in mlcroissant:\n",
    "\n",
    "| mlcroissant API | JSON-LD Schema | Description | Python Equivalent |\n",
    "|----------------|----------------|-------------|------------------|\n",
    "| `mlc.DataType.INTEGER` | `sc:Integer` | Integer values | `int` |\n",
    "| `mlc.DataType.FLOAT` | `sc:Float` | Floating-point values | `float` |\n",
    "| `mlc.DataType.TEXT` | `sc:Text` | String/text data | `str` |\n",
    "| `mlc.DataType.BOOL` | `sc:Boolean` | Boolean values | `bool` |\n",
    "| `mlc.DataType.DATE` | `sc:Date` | Date values | `datetime.date` |\n",
    "| `mlc.DataType.DATETIME` | `sc:DateTime` | Date and time values | `datetime.datetime` |\n",
    "\n",
    "**Note**: The mlcroissant Python library uses `mlc.DataType.FLOAT`, `mlc.DataType.INTEGER`, etc., which are converted to the JSON-LD schema types (`sc:Float`, `sc:Integer`) when calling `metadata.to_json()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c9edb",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Descriptive Naming**: Use clear, descriptive names for all fields and file objects\n",
    "2. **Complete Descriptions**: Provide detailed descriptions for each field\n",
    "3. **Specify Data Types**: Always specify the correct data type for each field\n",
    "4. **Use @id References**: Use @id to create references between components\n",
    "5. **Include Metadata**: Add license, URL, and other dataset-level metadata\n",
    "6. **Version Control**: Track versions of your Croissant metadata\n",
    "7. **Validate**: Always validate your Croissant file before sharing\n",
    "8. **Document Units**: Specify units for numerical fields in descriptions\n",
    "9. **Handle Missing Data**: Document how missing values are represented\n",
    "10. **Test Loading**: Verify the dataset can be loaded with mlcroissant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4cb5a",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **File Not Found**: Ensure all file paths in `contentUrl` and `containedIn` are correct\n",
    "2. **Invalid JSON**: Validate your JSON structure using a JSON linter\n",
    "3. **Missing @id**: All components that are referenced must have an `@id` field\n",
    "4. **Type Mismatches**: Ensure data types match the actual data in your files\n",
    "5. **Field Linking**: Verify that `source` references point to valid `@id` values\n",
    "\n",
    "### Debugging Tips:\n",
    "\n",
    "```python\n",
    "# Validate JSON structure\n",
    "import json\n",
    "with open('croissant.json') as f:\n",
    "    data = json.load(f)  # Will raise error if invalid JSON\n",
    "\n",
    "# Check required fields\n",
    "required = ['@context', '@type', 'name', 'distribution', 'recordSet']\n",
    "missing = [k for k in required if k not in data]\n",
    "if missing:\n",
    "    print(f\"Missing required fields: {missing}\")\n",
    "\n",
    "# Verify file paths exist\n",
    "from pathlib import Path\n",
    "for dist in data['distribution']:\n",
    "    if '@type' == 'ml:FileObject':\n",
    "        path = Path(dist['contentUrl'])\n",
    "        if not path.exists():\n",
    "            print(f\"Warning: File not found: {path}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b1285",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [Croissant Official Documentation](https://github.com/mlcommons/croissant)\n",
    "- [Schema.org Vocabulary](https://schema.org/)\n",
    "- [mlcroissant Python Library](https://pypi.org/project/mlcroissant/)\n",
    "- [MLCommons Croissant Tutorial](https://colab.research.google.com/github/mlcommons/croissant/blob/main/python/mlcroissant/recipes/introduction.ipynb)\n",
    "\n",
    "## Summary\n",
    "\n",
    "Converting a dataset to Croissant format involves:\n",
    "1. Understanding your dataset structure\n",
    "2. Defining file distribution (FileObject and FileSet)\n",
    "3. Creating RecordSet with appropriate Fields\n",
    "4. Specifying correct data types\n",
    "5. Linking fields to data sources\n",
    "6. Validating the metadata\n",
    "7. Testing dataset loading\n",
    "\n",
    "The Croissant format makes your dataset more discoverable, interoperable, and easier to use across different ML frameworks and platforms."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
