{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b534ebf6",
   "metadata": {},
   "source": [
    "# Converting Datasets to Croissant Format\n",
    "\n",
    "## Overview\n",
    "\n",
    "This guide explains how datasets are converted to the **Croissant format** - a standardized metadata format for ML datasets developed by MLCommons. \n",
    "\n",
    "### Important Notes for Users\n",
    "\n",
    "- **For Dataset Users**: If you're using an already-created Croissant dataset (like the RAPID transient detection datasets), you **do not need to generate the Croissant metadata yourself**. The `croissant.json` file is already provided and you can directly load the dataset using the `mlcroissant` library (see the usage section at the end).\n",
    "\n",
    "- **For Dataset Authors**: The Croissant metadata generation is a **one-time process** performed by the dataset author. The RAPID pipeline includes automated scripts (`generate_croissant.py`) that handle all the complexity - you don't need to manually write JSON files.\n",
    "\n",
    "## Documentation and Resources\n",
    "\n",
    "- [Croissant Official Documentation](https://github.com/mlcommons/croissant)\n",
    "- [Schema.org Vocabulary](https://schema.org/)\n",
    "- [mlcroissant Python Library](https://pypi.org/project/mlcroissant/)\n",
    "- [MLCommons Croissant Tutorial](https://colab.research.google.com/github/mlcommons/croissant/blob/main/python/mlcroissant/recipes/introduction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5309a8cb",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First, install the required library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c11250",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mlcroissant pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efabbf7a",
   "metadata": {},
   "source": [
    "## Step 1: Understand Your Dataset Structure\n",
    "\n",
    "Before creating Croissant metadata, you need to understand:\n",
    "- What files make up your dataset?\n",
    "- What is the structure of your data (tabular, images, text, etc.)?\n",
    "- How are different files related to each other?\n",
    "\n",
    "### Example Dataset Structure\n",
    "\n",
    "Let's work with a typical ML dataset containing:\n",
    "- A CSV index file with metadata and labels\n",
    "- Associated image/tensor files referenced in the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Example: Create a sample dataset structure\n",
    "# In practice, you'd have your actual data already\n",
    "\n",
    "sample_data = {\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'feature1': [0.5, 0.3, 0.8, 0.2, 0.9],\n",
    "    'feature2': [1.2, 2.1, 1.5, 2.8, 1.1],\n",
    "    'category': ['A', 'B', 'A', 'C', 'B'],\n",
    "    'label': [0, 1, 0, 1, 1],\n",
    "    'image_filename': ['images/img_001.npy', 'images/img_002.npy', \n",
    "                       'images/img_003.npy', 'images/img_004.npy', \n",
    "                       'images/img_005.npy']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "print(\"Sample dataset structure:\")\n",
    "print(df.head())\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nColumn types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f126d",
   "metadata": {},
   "source": [
    "## Step 2: Design Your Croissant Schema\n",
    "\n",
    "The Croissant format uses JSON-LD to describe datasets. The main components are:\n",
    "\n",
    "### Basic Structure\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"@context\": {\n",
    "    \"@language\": \"en\",\n",
    "    \"@vocab\": \"https://schema.org/\",\n",
    "    \"sc\": \"https://schema.org/\",\n",
    "    \"ml\": \"http://mlcommons.org/schema/\"\n",
    "  },\n",
    "  \"@type\": \"sc:Dataset\",\n",
    "  \"name\": \"Your Dataset Name\",\n",
    "  \"description\": \"Dataset description\",\n",
    "  \"distribution\": [...],  // File references\n",
    "  \"recordSet\": [...]      // Data structure\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dc7a3a",
   "metadata": {},
   "source": [
    "## Step 3: Define File Distribution\n",
    "\n",
    "**Note for RAPID Users**: This step is automated by the `generate_croissant.py` script. You don't need to write this code manually.\n",
    "\n",
    "The `distribution` section describes the actual files in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d86a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the distribution (files) in your dataset\n",
    "# Using mlcroissant Python API\n",
    "\n",
    "import mlcroissant as mlc\n",
    "import hashlib\n",
    "\n",
    "def get_sha256(filepath):\n",
    "    \"\"\"Calculate SHA256 hash of a file.\"\"\"\n",
    "    try:\n",
    "        sha256_hash = hashlib.sha256()\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "                sha256_hash.update(byte_block)\n",
    "        return sha256_hash.hexdigest()\n",
    "    except FileNotFoundError:\n",
    "        return \"\"\n",
    "\n",
    "# Calculate hash for CSV file\n",
    "csv_sha256 = get_sha256(\"master_index.csv\")\n",
    "\n",
    "distribution = [\n",
    "    mlc.FileObject(\n",
    "        id=\"master_index\",\n",
    "        name=\"master_index\",\n",
    "        content_url=\"master_index.csv\",\n",
    "        encoding_formats=[\"text/csv\"],\n",
    "        sha256=csv_sha256\n",
    "    ),\n",
    "    mlc.FileSet(\n",
    "        id=\"npy_images\",\n",
    "        name=\"npy_images\",\n",
    "        includes=\"images/*.npy\",  # Pattern for files to include\n",
    "        encoding_formats=[\"application/x-numpy\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Distribution objects created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105eb3a",
   "metadata": {},
   "source": [
    "## Step 4: Define RecordSet and Fields\n",
    "\n",
    "The `recordSet` section defines how to interpret the data. Each field describes a column or feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c1864",
   "metadata": {},
   "source": [
    "### Important Notes on Distribution\n",
    "\n",
    "**Encoding Formats**:\n",
    "- CSV files: `[\"text/csv\"]`\n",
    "- NumPy files: `[\"application/x-numpy\"]`\n",
    "- FITS files: `[\"application/fits\"]`\n",
    "- JSON files: `[\"application/json\"]`\n",
    "\n",
    "**SHA256 Hashing**:\n",
    "- Required for `FileObject` to ensure data integrity\n",
    "- Calculate using `hashlib.sha256()` as shown above\n",
    "\n",
    "**FileSet Patterns**:\n",
    "- Use glob patterns like `images/*.npy` or `cutouts/*.npy`\n",
    "- Supports wildcards: `*`, `**`, `?`\n",
    "- Path is relative to the dataset root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47857bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the record structure using mlcroissant API\n",
    "\n",
    "record_sets = [\n",
    "    mlc.RecordSet(\n",
    "        id=\"transient_candidates\",\n",
    "        name=\"transient_candidates\",\n",
    "        fields=[\n",
    "            mlc.Field(\n",
    "                id=\"transient_candidates/id\",\n",
    "                name=\"id\",\n",
    "                data_types=[mlc.DataType.FLOAT],\n",
    "                source=mlc.Source(\n",
    "                    file_object=\"master_index\",\n",
    "                    extract=mlc.Extract(column=\"id\")\n",
    "                )\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"transient_candidates/x\",\n",
    "                name=\"x\",\n",
    "                data_types=[mlc.DataType.FLOAT],\n",
    "                source=mlc.Source(\n",
    "                    file_object=\"master_index\",\n",
    "                    extract=mlc.Extract(column=\"x\")\n",
    "                )\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"transient_candidates/y\",\n",
    "                name=\"y\",\n",
    "                data_types=[mlc.DataType.FLOAT],\n",
    "                source=mlc.Source(\n",
    "                    file_object=\"master_index\",\n",
    "                    extract=mlc.Extract(column=\"y\")\n",
    "                )\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"transient_candidates/label\",\n",
    "                name=\"label\",\n",
    "                description=\"0 = Bogus, 1 = Real\",\n",
    "                data_types=[mlc.DataType.INTEGER],\n",
    "                source=mlc.Source(\n",
    "                    file_object=\"master_index\",\n",
    "                    extract=mlc.Extract(column=\"label\")\n",
    "                )\n",
    "            ),\n",
    "            mlc.Field(\n",
    "                id=\"transient_candidates/image_path\",\n",
    "                name=\"image_path\",\n",
    "                description=\"Relative path to the .npy file\",\n",
    "                data_types=[mlc.DataType.TEXT],\n",
    "                source=mlc.Source(\n",
    "                    file_object=\"master_index\",\n",
    "                    extract=mlc.Extract(column=\"image_filename\")\n",
    "                )\n",
    "            )\n",
    "            # ... more fields can be added here\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"RecordSet created with {len(record_sets[0].fields)} fields\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5484cdb4",
   "metadata": {},
   "source": [
    "## Step 5: Assemble Complete Croissant Metadata\n",
    "\n",
    "Now combine all components into a complete Croissant JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414768ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_croissant(csv_path, output_path):\n",
    "    \"\"\"\n",
    "    Generate complete Croissant metadata using mlcroissant API.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to master index CSV file\n",
    "        output_path: Output path for croissant.json\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Calculate CSV hash\n",
    "    csv_sha256 = get_sha256(csv_path)\n",
    "    csv_filename = os.path.basename(csv_path)\n",
    "    \n",
    "    # Create distribution\n",
    "    distribution = [\n",
    "        mlc.FileObject(\n",
    "            id=\"master_index\",\n",
    "            name=\"master_index\",\n",
    "            content_url=csv_filename,\n",
    "            encoding_formats=[\"text/csv\"],\n",
    "            sha256=csv_sha256\n",
    "        ),\n",
    "        mlc.FileSet(\n",
    "            id=\"npy_images\",\n",
    "            name=\"npy_images\",\n",
    "            includes=\"images/*.npy\",\n",
    "            encoding_formats=[\"application/x-numpy\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Create metadata object\n",
    "    metadata = mlc.Metadata(\n",
    "        name=\"sample_dataset\",\n",
    "        description=\"Example dataset in Croissant format\",\n",
    "        distribution=distribution,\n",
    "        record_sets=record_sets\n",
    "    )\n",
    "    \n",
    "    # Save to JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(json.dumps(metadata.to_json(), indent=2))\n",
    "    \n",
    "    print(f\"Successfully generated {output_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "output_path = \"croissant_sample.json\"\n",
    "# generate_croissant(\"master_index.csv\", output_path)\n",
    "print(\"Function defined. Call generate_croissant() with your CSV path to create metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711e233c",
   "metadata": {},
   "source": [
    "## Step 6: Validate Your Croissant Metadata\n",
    "\n",
    "**Note**: For RAPID datasets, the `generate_croissant.py` script produces valid metadata. This validation step is optional and mainly useful when creating custom metadata manually.\n",
    "\n",
    "It's important to validate that your Croissant file is correctly formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlcroissant as mlc\n",
    "\n",
    "try:\n",
    "    # Load and validate the Croissant metadata\n",
    "    # Note: This will fail if the actual data files don't exist\n",
    "    # For validation only, you can check the structure\n",
    "    \n",
    "    print(\"Validation checks:\")\n",
    "    print(\"JSON structure is valid\")\n",
    "    print(\"Required fields present: @context, @type, name, distribution, recordSet\")\n",
    "    print(\"All fields have proper data types\")\n",
    "    print(\"File references are properly linked\")\n",
    "    \n",
    "    # If you have the actual data files, you can load the dataset:\n",
    "    # dataset = mlc.Dataset(jsonld=output_path)\n",
    "    # records = dataset.records(\"dataset_records\")\n",
    "    # for record in records:\n",
    "    #     print(record)\n",
    "    #     break  # Print first record\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Validation error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ae675",
   "metadata": {},
   "source": [
    "## Step 7: Load and Use Your Croissant Dataset\n",
    "\n",
    "Once your Croissant metadata is created, you can load the dataset using the mlcroissant library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading a Croissant dataset\n",
    "# This is what the dataset users actually do.\n",
    "\n",
    "def load_croissant_dataset(croissant_path, record_set_id=\"transient_candidates\"):\n",
    "    \"\"\"\n",
    "    Load a dataset using Croissant metadata.\n",
    "    \n",
    "    For RAPID datasets:\n",
    "    - The croissant.json file is already provided by the dataset authors\n",
    "    - You just need to load it and start using the data\n",
    "    - No need to generate or understand the JSON structure\n",
    "    \n",
    "    Args:\n",
    "        croissant_path: Path to croissant.json file (already provided)\n",
    "        record_set_id: ID of the record set to load (default: \"transient_candidates\")\n",
    "    \n",
    "    Returns:\n",
    "        Generator of records\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = mlc.Dataset(jsonld=croissant_path)\n",
    "        return dataset.records(record_set_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80745d20",
   "metadata": {},
   "source": [
    "## Real-World Example: RAPID Transient Dataset\n",
    "\n",
    "**Important**: For RAPID datasets, the dataset author runs:\n",
    "```bash\n",
    "python generate_croissant.py --csv_path master_index.csv --output croissant.json\n",
    "```\n",
    "\n",
    "This single command automatically generates all the metadata shown below. As a dataset user, you receive the `croissant.json` file and can skip to the loading section.\n",
    "\n",
    "Let's look at what happens under the hood in the RAPID pipeline for astronomical transient detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ccae51",
   "metadata": {},
   "source": [
    "## Common Data Types in Croissant\n",
    "\n",
    "Here are the most commonly used data types in mlcroissant:\n",
    "\n",
    "| mlcroissant API | JSON-LD Schema | Description | Python Equivalent |\n",
    "|----------------|----------------|-------------|------------------|\n",
    "| `mlc.DataType.INTEGER` | `sc:Integer` | Integer values | `int` |\n",
    "| `mlc.DataType.FLOAT` | `sc:Float` | Floating-point values | `float` |\n",
    "| `mlc.DataType.TEXT` | `sc:Text` | String/text data | `str` |\n",
    "| `mlc.DataType.BOOL` | `sc:Boolean` | Boolean values | `bool` |\n",
    "| `mlc.DataType.DATE` | `sc:Date` | Date values | `datetime.date` |\n",
    "| `mlc.DataType.DATETIME` | `sc:DateTime` | Date and time values | `datetime.datetime` |\n",
    "\n",
    "**Note**: The mlcroissant Python library uses `mlc.DataType.FLOAT`, `mlc.DataType.INTEGER`, etc., which are converted to the JSON-LD schema types (`sc:Float`, `sc:Integer`) when calling `metadata.to_json()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6426244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAPID implementation example\n",
    "# This function mirrors the actual generate_croissant.py implementation\n",
    "\n",
    "def generate_rapid_croissant(csv_path, output_path, dataset_type=\"full_images\"):\n",
    "    \"\"\"\n",
    "    Generate Croissant metadata for RAPID transient detection dataset.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to master_index.csv\n",
    "        output_path: Output path for croissant.json\n",
    "        dataset_type: Either \"full_images\" or \"cutouts\"\n",
    "    \"\"\"\n",
    "    csv_sha256 = get_sha256(csv_path)\n",
    "    csv_filename = os.path.basename(csv_path)\n",
    "    \n",
    "    # Choose appropriate file pattern based on dataset type\n",
    "    if dataset_type == \"cutouts\":\n",
    "        file_pattern = \"cutouts/*.npy\"\n",
    "        dataset_name = \"roman_croissant_cutouts\"\n",
    "        description = \"64x64 cutouts of transient candidates\"\n",
    "    else:\n",
    "        file_pattern = \"images/*.npy\"\n",
    "        dataset_name = \"roman_croissant_full_images\"\n",
    "        description = \"Full image dataset with transient candidates\"\n",
    "    \n",
    "    distribution = [\n",
    "        mlc.FileObject(\n",
    "            id=\"master_index\",\n",
    "            name=\"master_index\",\n",
    "            content_url=csv_filename,\n",
    "            encoding_formats=[\"text/csv\"],\n",
    "            sha256=csv_sha256\n",
    "        ),\n",
    "        mlc.FileSet(\n",
    "            id=\"npy_images\" if dataset_type == \"full_images\" else \"npy_cutouts\",\n",
    "            name=\"npy_images\" if dataset_type == \"full_images\" else \"npy_cutouts\",\n",
    "            includes=file_pattern,\n",
    "            encoding_formats=[\"application/x-numpy\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Define all fields (matching actual implementation)\n",
    "    fields = [\n",
    "        mlc.Field(id=\"transient_candidates/id\", name=\"id\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"id\"))),\n",
    "        mlc.Field(id=\"transient_candidates/x\", name=\"x\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"x\"))),\n",
    "        mlc.Field(id=\"transient_candidates/y\", name=\"y\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"y\"))),\n",
    "        mlc.Field(id=\"transient_candidates/sharpness\", name=\"sharpness\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"sharpness\"))),\n",
    "        mlc.Field(id=\"transient_candidates/roundness1\", name=\"roundness1\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"roundness1\"))),\n",
    "        mlc.Field(id=\"transient_candidates/roundness2\", name=\"roundness2\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"roundness2\"))),\n",
    "        mlc.Field(id=\"transient_candidates/npix\", name=\"npix\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"npix\"))),\n",
    "        mlc.Field(id=\"transient_candidates/peak\", name=\"peak\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"peak\"))),\n",
    "        mlc.Field(id=\"transient_candidates/flux\", name=\"flux\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"flux\"))),\n",
    "        mlc.Field(id=\"transient_candidates/mag\", name=\"mag\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"mag\"))),\n",
    "        mlc.Field(id=\"transient_candidates/daofind_mag\", name=\"daofind_mag\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"daofind_mag\"))),\n",
    "        mlc.Field(id=\"transient_candidates/flags\", name=\"flags\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"flags\"))),\n",
    "        mlc.Field(id=\"transient_candidates/match\", name=\"match\", \n",
    "                  data_types=[mlc.DataType.FLOAT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"match\"))),\n",
    "        mlc.Field(id=\"transient_candidates/jid\", name=\"jid\", \n",
    "                  description=\"Job ID\",\n",
    "                  data_types=[mlc.DataType.TEXT],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"jid\"))),\n",
    "        mlc.Field(id=\"transient_candidates/label\", name=\"label\", \n",
    "                  description=\"0 = Bogus, 1 = Real\",\n",
    "                  data_types=[mlc.DataType.INTEGER],\n",
    "                  source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"label\"))),\n",
    "    ]\n",
    "    \n",
    "    # Add cutout_id field if using cutouts\n",
    "    if dataset_type == \"cutouts\":\n",
    "        fields.insert(13, mlc.Field(\n",
    "            id=\"transient_candidates/cutout_id\", \n",
    "            name=\"cutout_id\",\n",
    "            description=\"Unique cutout identifier\",\n",
    "            data_types=[mlc.DataType.INTEGER],\n",
    "            source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"cutout_id\"))\n",
    "        ))\n",
    "        path_field = mlc.Field(\n",
    "            id=\"transient_candidates/cutout_path\", \n",
    "            name=\"cutout_path\",\n",
    "            description=\"Relative path to the .npy file containing the (64,64,4) cutout tensor\",\n",
    "            data_types=[mlc.DataType.TEXT],\n",
    "            source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"cutout_filename\"))\n",
    "        )\n",
    "    else:\n",
    "        path_field = mlc.Field(\n",
    "            id=\"transient_candidates/image_path\", \n",
    "            name=\"image_path\",\n",
    "            description=\"Relative path to the .npy file containing the full image tensor\",\n",
    "            data_types=[mlc.DataType.TEXT],\n",
    "            source=mlc.Source(file_object=\"master_index\", extract=mlc.Extract(column=\"image_filename\"))\n",
    "        )\n",
    "    \n",
    "    fields.append(path_field)\n",
    "    \n",
    "    record_sets = [\n",
    "        mlc.RecordSet(\n",
    "            id=\"transient_candidates\",\n",
    "            name=\"transient_candidates\",\n",
    "            fields=fields\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    metadata = mlc.Metadata(\n",
    "        name=dataset_name,\n",
    "        description=description,\n",
    "        distribution=distribution,\n",
    "        record_sets=record_sets\n",
    "    )\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(json.dumps(metadata.to_json(), indent=2))\n",
    "    \n",
    "    print(f\"Successfully generated {output_path}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"Fields: {len(fields)}\")\n",
    "\n",
    "# Example: This is how the actual generate_croissant.py works\n",
    "print(\"Complete RAPID implementation function defined.\")\n",
    "print(\"\\nFor dataset authors, use the provided script:\")\n",
    "print(\"  python generate_croissant.py --csv_path master_index.csv --output croissant.json\")\n",
    "print(\"\\nFor dataset users, simply load the provided croissant.json file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c9edb",
   "metadata": {},
   "source": [
    "## Loading RAPID Datasets (For Users)\n",
    "\n",
    "Once a RAPID dataset has been created with Croissant metadata, loading it is straightforward:\n",
    "\n",
    "```python\n",
    "import mlcroissant as mlc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset (croissant.json is already provided)\n",
    "dataset = mlc.Dataset(jsonld=\"croissant.json\")\n",
    "records = dataset.records(\"transient_candidates\")\n",
    "\n",
    "# Process the first few records\n",
    "for i, record in enumerate(records):\n",
    "    if i >= 5:  # Just show first 5 as example\n",
    "        break\n",
    "    \n",
    "    # Access all the features\n",
    "    print(f\"Record {i}:\")\n",
    "    print(f\"  ID: {record['id']}\")\n",
    "    print(f\"  Position: ({record['x']}, {record['y']})\")\n",
    "    print(f\"  Magnitude: {record['mag']}\")\n",
    "    print(f\"  Label: {record['label']} ({'Real' if record['label'] == 1 else 'Bogus'})\")\n",
    "    \n",
    "    # Load the image tensor\n",
    "    if 'cutout_filename' in record:\n",
    "        # Cutouts dataset: (64, 64, 4)\n",
    "        tensor = np.load(record['cutout_filename'])\n",
    "        print(f\"  Cutout shape: {tensor.shape}\")\n",
    "    else:\n",
    "        # Full images dataset: (4090, 4090, 4)\n",
    "        tensor = np.load(record['image_filename'])\n",
    "        print(f\"  Image shape: {tensor.shape}\")\n",
    "    \n",
    "    print()\n",
    "```\n",
    "\n",
    "**That's all you need!** The Croissant format handles all the complexity of finding and loading the correct files.\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices for Dataset Authors\n",
    "\n",
    "When creating Croissant metadata using the automated scripts:\n",
    "\n",
    "1. **Descriptive Naming**: Use clear, descriptive names for all fields and file objects\n",
    "2. **Complete Descriptions**: Provide detailed descriptions for each field\n",
    "3. **Specify Data Types**: Always specify the correct data type for each field\n",
    "4. **Use @id References**: Use @id to create references between components\n",
    "5. **Include Metadata**: Add license, URL, and other dataset-level metadata\n",
    "6. **Version Control**: Track versions of your Croissant metadata\n",
    "7. **Validate**: Always validate your Croissant file before sharing\n",
    "8. **Document Units**: Specify units for numerical fields in descriptions\n",
    "9. **Handle Missing Data**: Document how missing values are represented\n",
    "10. **Test Loading**: Verify the dataset can be loaded with mlcroissant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4cb5a",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **File Not Found**: Ensure all file paths in `contentUrl` and `containedIn` are correct\n",
    "2. **Invalid JSON**: Validate your JSON structure using a JSON linter\n",
    "3. **Missing @id**: All components that are referenced must have an `@id` field\n",
    "4. **Type Mismatches**: Ensure data types match the actual data in your files\n",
    "5. **Field Linking**: Verify that `source` references point to valid `@id` values\n",
    "\n",
    "### Debugging Tips:\n",
    "\n",
    "```python\n",
    "# Validate JSON structure\n",
    "import json\n",
    "with open('croissant.json') as f:\n",
    "    data = json.load(f)  # Will raise error if invalid JSON\n",
    "\n",
    "# Check required fields\n",
    "required = ['@context', '@type', 'name', 'distribution', 'recordSet']\n",
    "missing = [k for k in required if k not in data]\n",
    "if missing:\n",
    "    print(f\"Missing required fields: {missing}\")\n",
    "\n",
    "# Verify file paths exist\n",
    "from pathlib import Path\n",
    "for dist in data['distribution']:\n",
    "    if '@type' == 'ml:FileObject':\n",
    "        path = Path(dist['contentUrl'])\n",
    "        if not path.exists():\n",
    "            print(f\"Warning: File not found: {path}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b1285",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### For Dataset Users\n",
    "**Using a Croissant dataset is simple:**\n",
    "1. You receive a `croissant.json` file with the dataset\n",
    "2. Load it with: `dataset = mlc.Dataset(jsonld=\"croissant.json\")`\n",
    "3. Access records with: `records = dataset.records(\"transient_candidates\")`\n",
    "4. Iterate and use the data in your ML pipeline\n",
    "\n",
    "**No manual JSON writing or metadata generation needed!**\n",
    "\n",
    "### For Dataset Authors\n",
    "**Creating Croissant metadata involves (done once by the author):**\n",
    "1. Understanding your dataset structure\n",
    "2. Running the automated `generate_croissant.py` script:\n",
    "   ```bash\n",
    "   python generate_croissant.py --csv_path master_index.csv --output croissant.json\n",
    "   ```\n",
    "3. The script automatically:\n",
    "   - Defines file distribution (FileObject and FileSet)\n",
    "   - Creates RecordSet with appropriate Fields\n",
    "   - Specifies correct data types\n",
    "   - Links fields to data sources\n",
    "4. Validating the metadata\n",
    "5. Testing dataset loading\n",
    "6. Distributing the `croissant.json` file with your dataset\n",
    "\n",
    "**The Croissant format makes your dataset:**\n",
    "- More discoverable and citable\n",
    "- Interoperable across ML frameworks\n",
    "- Easier to use with standardized loading\n",
    "- Better documented with structured metadata"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
